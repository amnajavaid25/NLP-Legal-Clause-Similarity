{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82059ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 150881 clauses from 395 categories.\n",
      "Vocabulary size: 45462\n",
      "Classes: 395\n",
      "Creating positive/negative pairs...\n",
      "Created 200000 pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dilaw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:915: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dilaw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:915: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dilaw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:915: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BiLSTM...\n",
      "Epoch 1/3\n",
      "\u001b[1m 76/200\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m13:18\u001b[0m 6s/step - accuracy: 0.5047 - loss: 0.6926"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses, metrics\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# -------------------------- Reproducibility --------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# -------------------------- 1. DATA LOADING --------------------------\n",
    "def load_legal_clause_dataset(root_path: str):\n",
    "    data = []\n",
    "    for filename in os.listdir(root_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            category = filename.replace(\".csv\", \"\")\n",
    "            fp = os.path.join(root_path, filename)\n",
    "            df = pd.read_csv(fp)\n",
    "            text_col = next((c for c in df.columns if c.lower() in [\"clause\", \"text\", \"clauses\", \"content\"]), df.columns[0])\n",
    "            texts = df[text_col].astype(str).tolist()\n",
    "            data.extend([(text, category) for text in texts])\n",
    "    print(f\"Loaded {len(data)} clauses from {len([f for f in os.listdir(root_path) if f.endswith('.csv')])} categories.\")\n",
    "    return data\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s\\.,;:\\(\\)\\[\\]\\{\\}/\\\\\\'\"-]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "DATASET_PATH = r\"C:\\Users\\dilaw\\.cache\\kagglehub\\datasets\\bahushruth\\legalclausedataset\\versions\\1\"\n",
    "raw_data = load_legal_clause_dataset(DATASET_PATH)\n",
    "cleaned_data = [(clean_text(text), label) for text, label in raw_data]\n",
    "\n",
    "texts_only = [t for t, _ in cleaned_data]\n",
    "labels_only = [l for _, l in cleaned_data]\n",
    "\n",
    "# -------------------------- 2. TOKENIZATION --------------------------\n",
    "VOCAB_SIZE = 30000\n",
    "MAX_LEN = 256\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=VOCAB_SIZE,\n",
    "    oov_token=\"<OOV>\",\n",
    "    lower=True\n",
    ")\n",
    "tokenizer.fit_on_texts(texts_only)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(texts_only)\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_cat = label_encoder.fit_transform(labels_only)\n",
    "y = tf.keras.utils.to_categorical(y_cat)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Classes: {num_classes}\")\n",
    "\n",
    "# -------------------------- 3. CREATE PAIRS --------------------------\n",
    "def create_pairs(X, y, num_pairs=200_000):\n",
    "    print(\"Creating positive/negative pairs...\")\n",
    "    pairs, labels = [], []\n",
    "    y_arg = np.argmax(y, axis=1)\n",
    "    label_dict = {}\n",
    "    for idx, lbl in enumerate(y_arg):\n",
    "        label_dict.setdefault(lbl, []).append(idx)\n",
    "\n",
    "    for _ in range(num_pairs):\n",
    "        if random.random() < 0.5:  # positive\n",
    "            lbl = random.choice(list(label_dict.keys()))\n",
    "            if len(label_dict[lbl]) < 2:\n",
    "                continue\n",
    "            i, j = random.sample(label_dict[lbl], 2)\n",
    "            pairs.append((X[i], X[j]))\n",
    "            labels.append(1)\n",
    "        else:  # negative\n",
    "            lbl1, lbl2 = random.sample(list(label_dict.keys()), 2)\n",
    "            i = random.choice(label_dict[lbl1])\n",
    "            j = random.choice(label_dict[lbl2])\n",
    "            pairs.append((X[i], X[j]))\n",
    "            labels.append(0)\n",
    "\n",
    "    pairs = np.array(pairs)\n",
    "    labels = np.array(labels)\n",
    "    print(f\"Created {len(pairs)} pairs.\")\n",
    "    return pairs[:, 0], pairs[:, 1], labels\n",
    "\n",
    "X1, X2, pair_labels = create_pairs(X, y, num_pairs=200_000)\n",
    "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(\n",
    "    X1, X2, pair_labels, test_size=0.2, stratify=pair_labels, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------- 4. DATASET --------------------------\n",
    "def create_pair_dataset(X1, X2, y, batch_size=64):\n",
    "    def pair_generator():\n",
    "        n = len(X1)\n",
    "        idx = np.arange(n)\n",
    "        while True:\n",
    "            np.random.shuffle(idx)\n",
    "            for i in range(0, n, batch_size):\n",
    "                j = idx[i:i+batch_size]\n",
    "                yield {'anchor': X1[j], 'other': X2[j]}, y[j]\n",
    "\n",
    "    output_signature = (\n",
    "        {'anchor': tf.TensorSpec(shape=(None, MAX_LEN), dtype=tf.int32),\n",
    "         'other': tf.TensorSpec(shape=(None, MAX_LEN), dtype=tf.int32)},\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "    )\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(pair_generator, output_signature=output_signature)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = create_pair_dataset(X1_train, X2_train, y_train)\n",
    "val_ds = create_pair_dataset(X1_val, X2_val, y_val)\n",
    "\n",
    "# -------------------------- 5. MODEL BUILDERS --------------------------\n",
    "EMBED_DIM = 128\n",
    "DROPOUT = 0.3\n",
    "\n",
    "def get_embedding():\n",
    "    return layers.Embedding(VOCAB_SIZE, EMBED_DIM, mask_zero=True)\n",
    "\n",
    "class BiLSTMEncoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = get_embedding()\n",
    "        self.bilstm = layers.Bidirectional(layers.LSTM(128, dropout=DROPOUT, return_sequences=False))\n",
    "        self.dense = layers.Dense(64, activation='relu')\n",
    "        self.drop = layers.Dropout(DROPOUT)\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.bilstm(x)\n",
    "        x = self.dense(x)\n",
    "        return self.drop(x)\n",
    "\n",
    "class AttentionEncoder(tf.keras.Model):\n",
    "    def __init__(self, heads=8):\n",
    "        super().__init__()\n",
    "        self.emb = get_embedding()\n",
    "        self.mha = layers.MultiHeadAttention(num_heads=heads, key_dim=EMBED_DIM//heads)\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ffn = models.Sequential([layers.Dense(256, activation='relu'), layers.Dense(EMBED_DIM)])\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.pool = layers.GlobalAveragePooling1D()\n",
    "        self.out = layers.Dense(64, activation='relu')\n",
    "        self.drop = layers.Dropout(DROPOUT)\n",
    "    def call(self, x):\n",
    "        e = self.emb(x)\n",
    "        a = self.mha(e, e)\n",
    "        x = self.norm1(e + a)\n",
    "        f = self.ffn(x)\n",
    "        x = self.norm2(x + f)\n",
    "        x = self.pool(x)\n",
    "        x = self.out(x)\n",
    "        return self.drop(x)\n",
    "\n",
    "def build_siamese(encoder_class):\n",
    "    input_a = layers.Input(shape=(MAX_LEN,), name='anchor')\n",
    "    input_b = layers.Input(shape=(MAX_LEN,), name='other')\n",
    "    encoder = encoder_class()\n",
    "    enc_a = encoder(input_a)\n",
    "    enc_b = encoder(input_b)\n",
    "    distance = layers.Lambda(lambda t: K.abs(t[0] - t[1]))([enc_a, enc_b])\n",
    "    output = layers.Dense(1, activation='sigmoid')(distance)\n",
    "    model = models.Model(inputs={'anchor': input_a, 'other': input_b}, outputs=output)\n",
    "    return model\n",
    "\n",
    "bilstm_model = build_siamese(BiLSTMEncoder)\n",
    "attn_model = build_siamese(AttentionEncoder)\n",
    "\n",
    "# -------------------------- 6. TRAIN FUNCTION --------------------------\n",
    "def train_model(model, name, epochs=3, steps_train=200, steps_val=50):\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(1e-3),\n",
    "        loss=losses.BinaryCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print(f\"Training {name}...\")\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        steps_per_epoch=steps_train,\n",
    "        validation_steps=steps_val,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    model.save_weights(f\"{name}_weights.h5\")\n",
    "    return history\n",
    "\n",
    "# -------------------------- 7. TRAIN BOTH --------------------------\n",
    "bilstm_hist = train_model(bilstm_model, \"BiLSTM\", epochs=3)\n",
    "attn_hist = train_model(attn_model, \"Attention\", epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddec069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
